<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Understanding Transfer Learning in Computer Vision Tasks - Tarun Ram</title><meta name="description" content="Neural Networks are now widely used in a variety of domains, including complex computer vision tasks such as object classification, semantic segmentation, and object detection. However, Neural Networks require large amounts of labelled data(millions of data samples) to acheive state-of-the-art performance when being trained from&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://peppermenta.github.io/publii-blog/understanding-transfer-learning-in-computer-vision-tasks.html"><link rel="alternate" type="application/atom+xml" href="https://peppermenta.github.io/publii-blog/feed.xml"><link rel="alternate" type="application/json" href="https://peppermenta.github.io/publii-blog/feed.json"><meta property="og:title" content="Understanding Transfer Learning in Computer Vision Tasks"><meta property="og:site_name" content="Tarun Ram"><meta property="og:description" content="Neural Networks are now widely used in a variety of domains, including complex computer vision tasks such as object classification, semantic segmentation, and object detection. However, Neural Networks require large amounts of labelled data(millions of data samples) to acheive state-of-the-art performance when being trained from&hellip;"><meta property="og:url" content="https://peppermenta.github.io/publii-blog/understanding-transfer-learning-in-computer-vision-tasks.html"><meta property="og:type" content="article"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://peppermenta.github.io/publii-blog/assets/css/style.css?v=05ae0ae2b389f01b91cdf8a8e44e6817"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://peppermenta.github.io/publii-blog/understanding-transfer-learning-in-computer-vision-tasks.html"},"headline":"Understanding Transfer Learning in Computer Vision Tasks","datePublished":"2021-05-30T21:30","dateModified":"2021-05-30T21:30","description":"Neural Networks are now widely used in a variety of domains, including complex computer vision tasks such as object classification, semantic segmentation, and object detection. However, Neural Networks require large amounts of labelled data(millions of data samples) to acheive state-of-the-art performance when being trained from&hellip;","author":{"@type":"Person","name":"Tarun Ram"},"publisher":{"@type":"Organization","name":"Tarun Ram"}}</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://peppermenta.github.io/publii-blog/">Tarun Ram</a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2021-05-30T21:30">May 30, 2021</time></div><h1>Understanding Transfer Learning in Computer Vision Tasks</h1><div class="post__meta post__meta--author"><a href="https://peppermenta.github.io/publii-blog/authors/tarun-ram/" class="feed__author invert">Tarun Ram</a></div></div></header></div><div class="wrapper post__entry"><p>Neural Networks are now widely used in a variety of domains, including complex computer vision tasks such as object classification, semantic segmentation, and object detection. However, Neural Networks require large amounts of <strong>labelled</strong> data(millions of data samples) to acheive state-of-the-art performance when being trained from scratch. For most tasks, these amounts of labelled data are not available, and not trivial to obtain. Training a large neural network from scratch is also computationally expensive, as well as time consuming.</p><p>Transfer learning allows us to leverage a pre-trained model as the starting point for the new model for the task at hand. Today, almost all networks are trained this way. Starting from a pre-trained network, we can either fine-tune its weights, freeze some layers and fine-tune the last few layers, or replace the final layers of the network to match the target task’s output. However, choosing a source model that transfers well to the target task is a finicky task, when the source task or source dataset differ.</p><p>I had entered a computer vision hackathon a couple of months ago, along with a few fellow students who had little-to-no experience in model training. The task at hand was image classification. Since the dataset provided was extremely small, it was evident that training a model from scratch would not result in good test performance. Transfer learning seemed to be the way to go. I felt the best plan was to take a state-of-the-art CNN trained on Imagenet, swap out the fully connected layers, and retrain these layers on the given dataset. However, explaining this notion to my teammates proved a challenge. Why would an un-related dataset like ImageNet work for this task? Choosing a source model is mainly based off of heuristics such as ‘The features learned by the model will be general enough’, ‘The source task is similar/same as the target task’ or ‘This dataset is similar to the target dataset’, which, while intuitive, do not have any theoretical support. I was unable to provide a strong logical statement supporting my choice of source model. There is no clear framework to determine how to best choose a source model, or how to leverage knowledge from different tasks to improve performance on the task at had. To develop a framework of this sort, a deeper dive into Transfer Learning is required.</p><p>Commonly in transfer learning, we take the activations from an intermediate layer of a network(typically called the ‘encoder’) trained on a different task. These are the <em>embeddings</em>, generally viewed as a representation for the inputs learnt by the network. These embeddings are then passed through a smaller network, called the <em>decoder</em>, to obtain the final outputs. Since the decoder is a smaller network, it can be trained using the target dataset. For example, for an image classification task, the encoder can be all the convolutional layers of a state-of-the-art CNN pre-trained on ImageNet, and the decoder can be a two layer fully connected neural network.</p><p>The aim of this project is to understand how to best utilize knowledge from various computer vision tasks while using transfer learning to improve performance on any downstream task(s). Models trained on different tasks show varying levels of success when used as an encoder for a different task. We aim to get a better understanding of why encoders trained on different tasks generate different embeddings. It was observed in the <a href="http://taskonomy.stanford.edu/" title="Link to Paper">Taskonomy</a> paper that networks pre-trained on certain tasks acheive good performance when used as a knowledge base for a wide range of tasks, while others do not perform well when used as a knowledge base for any different task. What makes certain tasks a better source for transfer learning? Are the features learnt by these networks more ‘general’? Are networks for different tasks focussing on different scale features in images? Is there a way to combine the learnings from various tasks? We hope that by answering these questions, it will be possible to develop a clear-cut framework to optimally leverage knowledge from various tasks/models to improve the performance of the task at hand</p><p>To tackle the initial problem of understanding of the differences in embeddedings from various tasks, we plan to use <a href="https://distill.pub/2019/activation-atlas/" title="Link to Article">Activation Atlases</a>. By comparing the activation atlases for encoders trained on various tasks(keeping the dataset the same), we hope to gain insights into the differences between the embeddings produced by various task-specific encoders. To prevent the alignment issue while comparing embeddings from different encoders, we will plot the activation atlas of the <em>feature visualizations</em> of the embedding layer after dimensionality reduction.</p><p>We wish to understand the relationship between various tasks(and their embeddings) by carefully analyzing the activation atlases. We hope to observe informative patters/clusters. For example, if a certain task X has embeddings that are well spread out, covering the spaces occupied by a large number of tasks, this may indicate that X will transfer well to a wide range of tasks, making a good default choice as a source task. Similarly, tasks whose clusters appear in overlapping regions may have similar feature representations, and hence will transfer well among each other. Isolating the spatial regions which maximally activate for each task’s embedding may explain if the corresponding task is more focused on small-scale or large-scale features. Basically, the activation atlas will allow us to view the various tasks in <em>embedding space</em>, giving us the opportunity to compare the tasks and make meaningful conclusions</p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on May 30, 2021</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://peppermenta.github.io/publii-blog/authors/tarun-ram/" class="invert" rel="author">Tarun Ram</a></h3></div></div></footer></article></main><footer class="footer"><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://peppermenta.github.io/publii-blog/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://peppermenta.github.io/publii-blog/assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>